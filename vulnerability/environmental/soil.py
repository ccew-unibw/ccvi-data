import numpy as np
from panel_imputer import PanelImputer
import pandas as pd
from tqdm import tqdm
import xarray as xr

from base.datasets import SoilErosionData, ESDACData
from base.objects import Indicator, ConfigParser, GlobalBaseGrid
from utils.conversions import pgid_to_coords
from utils.data_processing import create_custom_data_structure, min_max_scaling


class VulEnvironmentalSoil(Indicator):
    requires_processing_storage = True

    def __init__(
        self,
        config: ConfigParser,
        grid: GlobalBaseGrid,
        pillar: str = "VUL",
        dim: str = "environmental",
        id: str = "soil",
    ):
        """Params defining indicator's place in index set to designed hierarchy by default"""
        self.erosion = SoilErosionData(config=config)
        self.esdac = ESDACData(config=config)
        super().__init__(pillar=pillar, dim=dim, id=id, config=config, grid=grid)

    def load_data(self) -> tuple[xr.DataArray, xr.DataArray]:
        """"""
        da_erosion = self.erosion.load_data()
        da_carbon = self.esdac.load_data()
        return da_erosion, da_carbon

    def preprocess_data(self, input_das: tuple[xr.DataArray, xr.DataArray]) -> pd.DataFrame:
        """
        Runs preprocessing, creates grid aggregates, merges everything to the
        standard data structure with custom time boundaries, and fills nan for
        mean calculation.

        Caches results in processing storage since the layers are static.
        """
        # this needs to run either way since this sets the threshold for carbon debt
        da_erosion, da_carbon = input_das
        da_carbon = self.esdac.preprocess_data(da_carbon)
        filename = "soil_preprocessed"
        try:
            if self.regenerate["preprocessing"]:
                raise FileNotFoundError
            df_preprocessed = self.storage.load("processing", filename)
            self.console.print("Existing preprocessed soil data loaded from storage.")
        except FileNotFoundError:
            # 50.0 is the 99th percentile of da_erosion
            # hardcoded to save resources, since the data is static and percentile calculation on 6 bio pixels...
            df_erosion = self._create_grid_aggregates(da_erosion, "soil_erosion", 50, 2010)
            df_carbon = self._create_grid_aggregates(
                da_carbon, "carbon_debt", self.esdac.upper_threshold, 2020
            )
            df_base = create_custom_data_structure(grid.load(), 2010, 2020)
            df_preprocessed = pd.concat(
                [df_base, df_erosion["soil_erosion"], df_carbon["carbon_debt"]],
                join="outer",
                axis=1,
            )
            imputer = PanelImputer(
                time_index=["year", "quarter"],
                location_index="pgid",
                imputation_method="fill_all",
            )
            df_preprocessed: pd.DataFrame = imputer.fit_transform(df_preprocessed)  # type: ignore
            self.storage.save(df_preprocessed, "processing", filename)
        return df_preprocessed

    def create_indicator(self, df_preprocessed: pd.DataFrame) -> pd.DataFrame:
        df_indicator = self.create_base_df()
        # 50 is hardcoded percentile, see comment in preprocess_data()
        df_preprocessed["soil_erosion"] = df_preprocessed["soil_erosion"] / 50
        df_preprocessed["carbon_debt"] = df_preprocessed["carbon_debt"] / self.esdac.upper_threshold
        df_indicator = pd.merge(
            df_indicator,
            df_preprocessed[["soil_erosion", "carbon_debt"]],
            how="left",
            left_index=True,
            right_index=True,
        )
        df_indicator[self.composite_id] = df_indicator[["soil_erosion", "carbon_debt"]].mean(axis=1)
        return df_indicator

    def normalize(self, df: pd.DataFrame) -> pd.DataFrame:
        # no need to set reference period since static data
        threshold = df[self.composite_id].quantile(0.99)
        df[self.composite_id] = min_max_scaling(df[self.composite_id], maxv=threshold)
        return df[[c for c in df.columns if self.composite_id in c]]

    def _create_grid_aggregates(self, da: xr.DataArray, name: str, limit: float, data_year: int):
        """Spatially aggregates raster data to the 0.5-degree grid.

        Takes an xarray DataArray and calculates the mean pixel value within
        each 0.5-degree grid cell defined by `self.grid`. Prior to calculating
        the mean, it clips the source pixel values at a specified upper `limit`
        to handle extreme outliers. The resulting static (single-year) data is
        then assigned to a specific year and quarter for merging with the indicator
        panel data structure.

        Args:
            da (xr.DataArray): The input raster data to be aggregated.
            name (str): The desired column name for the aggregated values in the
                output DataFrame.
            limit (float): The upper threshold at which to cap the source pixel
                values before aggregation.
            data_year (int): The year of the static source data.

        Returns:
            pd.DataFrame: A DataFrame indexed by ('pgid', 'year', 'quarter')
                containing the aggregated values in a column named after `name`.
        """

        def aggregate_vals(pgid):
            lat, lon = pgid_to_coords(pgid)
            da_cell = da.sel(y=slice(lat + 0.25, lat - 0.25), x=slice(lon - 0.25, lon + 0.25))
            if da_cell.size == 0:
                return np.nan
            elif da_cell.isnull().all():
                return np.nan
            else:
                da_cell = da_cell.clip(max=limit)
                return da_cell.mean().item()

        df = self.grid.load()
        self.console.print(f"Calculating {name} grid aggregates...")
        tqdm.pandas()
        df[name] = df.reset_index()["pgid"].progress_apply(aggregate_vals).to_list()
        # all static year data, set to first quarter of start_year if later
        start_year = self.global_config["start_year"]
        year = start_year if start_year > data_year else data_year
        df["year"] = year
        df["quarter"] = 1 if year > data_year else 4
        df = df.set_index(["year", "quarter"], append=True).sort_index()
        return df


if __name__ == "__main__":
    config = ConfigParser()
    grid = GlobalBaseGrid(config)
    indicator = VulEnvironmentalSoil(config=config, grid=grid)
    indicator.run()

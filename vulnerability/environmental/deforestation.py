import multiprocessing
import os
import time
from typing import Literal

from joblib import Parallel, delayed
import numpy as np
from panel_imputer import PanelImputer
import pandas as pd
from rich.progress import Progress, TimeElapsedColumn
from tqdm import tqdm
import xarray as xr
import rioxarray as rxr

from base.datasets import GFCData
from base.objects import Indicator, ConfigParser, GlobalBaseGrid
from utils.data_processing import default_impute, min_max_scaling
from utils.spatial_operations import pgid_to_coords


class VulEnvironmentalDeforestation(Indicator):
    requires_processing_storage = True

    def __init__(
        self,
        config: ConfigParser,
        grid: GlobalBaseGrid,
        pillar: str = "VUL",
        dim: str = "environmental",
        id: str = "deforestation",
    ):
        """Params defining indicator's place in index set to designed hierarchy by default"""
        self.gfc = GFCData(config=config)
        super().__init__(pillar=pillar, dim=dim, id=id, config=config, grid=grid)

    def load_data(self) -> None:
        self.gfc.load_data(self.grid)
        return

    def preprocess_data(self, *args, **kwargs) -> None:
        """No preprocessing necessary"""
        assert self.gfc.data_loaded
        return

    def create_indicator(self, *args, **kwargs) -> pd.DataFrame:
        df = self.create_base_df()
        years = list(range(self.global_config["start_year"] - 1, int(self.gfc.version[4:8]) + 1))
        processed_filename = "deforestation_raw_imputed"
        try:
            if self.regenerate["preprocessing"]:
                raise FileNotFoundError
            df_prior = self.storage.load("processing", processed_filename)
            if not df_prior.dropna().reset_index()["year"].max() == max(years):
                raise FileNotFoundError
            self.console.print(
                "Existing indicator values for latest GFC version loaded from storage. No further processing required."
            )
            df = df.merge(df_prior, how="left", left_index=True, right_index=True)
        except FileNotFoundError:
            df[f"{self.composite_id}_raw"] = np.nan
            file_dict = self.gfc.get_dict_files(self.grid)
            all_pgids = df.index.get_level_values("pgid").unique()
            with Progress(
                *Progress.get_default_columns(),
                TimeElapsedColumn(),
                console=self.console,
                speed_estimate_period=120,
            ) as progress:
                processing_task = progress.add_task(
                    "Calculating grid tree loss aggregates", total=len(file_dict)
                )
                with Parallel(n_jobs=-2) as parallel:
                    for tile in file_dict:
                        pgids_tile = self.gfc.get_pgids_for_tile(tile, all_pgids)
                        assert pgids_tile is not None
                        ds = self._combine_tile_data(file_dict[tile])
                        if ds is None:
                            # df column already initialized to np.nan
                            pass
                        elif type(ds) is str and ds == "all_zero":
                            df.loc[
                                (pgids_tile, slice(None, max(years)), slice(None)),
                                f"{self.composite_id}_raw",
                            ] = 0
                            self.console.print(
                                "No loss data available despite treecover data.. assigning no loss to all pgids in tile:",
                                tile,
                            )
                        else:
                            losses = parallel(
                                delayed(self._get_cell_tree_loss_share)(pgid, years, ds)
                                for pgid in pgids_tile
                            )
                            for i, pgid in enumerate(pgids_tile):
                                for year in losses[i]:
                                    # assign to last quarter of year and impute between later
                                    df.at[(pgid, year, 4), f"{self.composite_id}_raw"] = losses[i][
                                        year
                                    ]
                        progress.update(processing_task, advance=1)
            # there is a bug I can't figure out where the imputer's joblib Parallel does not appear
            # to work and imputation takes incredibly long (days)
            imputer = PanelImputer(
                time_index=["year", "quarter"],
                location_index="pgid",
                imputation_method="interpolate",
                interp_method="slinear",
                tail_behavior="None",
                parallel_kwargs={"n_jobs": 16, "verbose": 1},
            )
            df: pd.DataFrame = imputer.fit_transform(df)  # type: ignore
            # remove extra year after imputation
            df = df.loc[
                (slice(None), slice(self.global_config["start_year"], None), slice(None)),
                slice(None),
            ]
            self.storage.save(df[[f"{self.composite_id}_raw"]], "processing", processed_filename)
        return df

    def normalize(self, df_indicator: pd.DataFrame) -> pd.DataFrame:
        threshold = df_indicator.loc[
            (slice(None), slice(None, 2020)), f"{self.composite_id}_raw"
        ].quantile(0.99)
        df_indicator[self.composite_id] = min_max_scaling(
            df_indicator[f"{self.composite_id}_raw"],
            maxv=threshold,
        )
        return df_indicator[[c for c in df_indicator.columns if self.composite_id in c]]

    @staticmethod
    def _combine_tile_data(
        tile_files: dict[str, str | None] | None,
    ) -> xr.Dataset | None | Literal["all_zero"]:
        # no data available
        if tile_files is None:
            return None
        if tile_files["lossyear"] is None:
            # no loss and no treecover is coded as NA - no action
            if rxr.open_rasterio(tile_files["treecover2000"]).sum() != 0:
                # no loss data but treecover gets a zero (just one case in the data)
                return "all_zero"
            else:
                return None
        # read & combine tile data
        assert all(layer is not None for layer in tile_files), "Unexpected None in file_dict"
        das = [rxr.open_rasterio(tile_files[layer]).rename(layer) for layer in tile_files]  # type: ignore
        ds = xr.merge(das, compat="identical").load()
        return ds

    @staticmethod
    def _get_cell_tree_loss_share(pgid: int, years: list[int], ds: xr.Dataset) -> dict[int, float]:
        y, x = pgid_to_coords(pgid)
        ds_cell = ds.sel(x=slice(x - 0.25, x + 0.25), y=slice(y + 0.25, y - 0.25))
        # percent of grid cell land covered by trees
        cell_cover = ds_cell.treecover2000.where(ds_cell.datamask == 1).mean() / 100
        yearly_losses = {}
        for year in years:
            # dummy loss layer
            losses = xr.where(
                (ds_cell.lossyear > 0) & (ds_cell.lossyear <= int(str(year)[2:])), 1, 0
            )
            # account for differently covered pixels
            treecover_lost = (losses * ds.treecover2000).sum() / ds_cell.treecover2000.sum()
            # use percent cover as adjustment factor for cell
            treecover_adjusted = treecover_lost * cell_cover
            yearly_losses[year] = treecover_adjusted.item()
        return yearly_losses


if __name__ == "__main__":
    multiprocessing.set_start_method("spawn", force=True)
    config = ConfigParser()
    grid = GlobalBaseGrid(config)
    indicator = VulEnvironmentalDeforestation(config=config, grid=grid)
    indicator.run()

from typing import Literal

from joblib import Parallel, delayed
import numpy as np
import pandas as pd
from rich.progress import Progress, TimeElapsedColumn
import xarray as xr
import rioxarray as rxr

from base.datasets import GFCData
from base.objects import Indicator, ConfigParser, GlobalBaseGrid
from utils.data_processing import default_impute, min_max_scaling
from utils.conversions import pgid_to_coords


class VulEnvironmentalDeforestation(Indicator):
    requires_processing_storage = True

    def __init__(
        self,
        config: ConfigParser,
        grid: GlobalBaseGrid,
        pillar: str = "VUL",
        dim: str = "environmental",
        id: str = "deforestation",
    ):
        """Params defining indicator's place in index set to designed hierarchy by default"""
        self.gfc = GFCData(config=config)
        super().__init__(pillar=pillar, dim=dim, id=id, config=config, grid=grid)

    def load_data(self) -> None:
        self.gfc.load_data(self.grid)
        return

    def preprocess_data(self, *args, **kwargs) -> None:
        """No preprocessing necessary"""
        assert self.gfc.data_loaded
        return

    def create_indicator(self, *args, **kwargs) -> pd.DataFrame:
        years = list(range(self.global_config["start_year"] - 1, int(self.gfc.version[4:8]) + 1))
        processed_filename = "deforestation_raw_imputed"
        try:
            if self.regenerate["preprocessing"]:
                raise FileNotFoundError
            df_prior = self.storage.load("processing", processed_filename)
            if not df_prior.dropna().reset_index()["year"].max() == max(years):
                raise FileNotFoundError
            df = self.create_base_df()
            self.console.print(
                "Existing indicator values for latest GFC version loaded from storage.",
                "No further processing required.",
            )
            df = df.merge(df_prior, how="left", left_index=True, right_index=True)
        except FileNotFoundError:
            df = self.create_base_df(self.global_config["start_year"] - 1)
            df[f"{self.composite_id}_raw"] = np.nan
            file_dict = self.gfc.get_dict_files(self.grid)
            all_pgids = list(df.index.get_level_values("pgid").unique())
            with Parallel(n_jobs=-2) as parallel:
                with Progress(
                    *Progress.get_default_columns(),
                    TimeElapsedColumn(),
                    console=self.console,
                    speed_estimate_period=120,
                ) as progress:
                    processing_task = progress.add_task(
                        "Calculating grid tree loss aggregates", total=len(file_dict)
                    )
                    for tile in file_dict:
                        pgids_tile = self.gfc.get_pgids_for_tile(tile, all_pgids)
                        assert pgids_tile is not None
                        ds = self._combine_tile_data(file_dict[tile])
                        if ds is None:
                            # df column already initialized to np.nan
                            pass
                        elif type(ds) is str and ds == "all_zero":
                            df.loc[
                                (pgids_tile, slice(None, max(years)), slice(None)),
                                f"{self.composite_id}_raw",
                            ] = 0
                            self.console.print(
                                "Tile:",
                                tile,
                                "No loss data available despite treecover data.",
                                "Assigning no loss to all pgids in tile.",
                            )
                        else:
                            losses: list[dict[int, float]] = parallel(
                                delayed(self._get_cell_tree_loss_share)(pgid, years, ds)
                                for pgid in pgids_tile
                            )  # type: ignore
                            for i, pgid in enumerate(pgids_tile):
                                for year in losses[i]:
                                    # assign to last quarter of year and impute between later
                                    df.at[(pgid, year, 4), f"{self.composite_id}_raw"] = losses[i][
                                        year
                                    ]
                        progress.update(processing_task, advance=1)

            df = default_impute(df)
            # remove extra year after imputation
            df = df.loc[
                (slice(None), slice(self.global_config["start_year"], None), slice(None)),
                slice(None),
            ]
            self.storage.save(df[[f"{self.composite_id}_raw"]], "processing", processed_filename)
        return df

    def normalize(self, df_indicator: pd.DataFrame) -> pd.DataFrame:
        threshold = df_indicator.loc[
            (slice(None), slice(None, 2020)), f"{self.composite_id}_raw"
        ].quantile(0.99)
        df_indicator[self.composite_id] = min_max_scaling(
            df_indicator[f"{self.composite_id}_raw"],
            maxv=threshold,
        )
        return df_indicator[[c for c in df_indicator.columns if self.composite_id in c]]

    @staticmethod
    def _combine_tile_data(
        tile_files: dict[str, str | None] | None,
    ) -> xr.Dataset | None | Literal["all_zero"]:
        """Loads and merges the raster layers for a single Global Forest Change tile.

        This static helper method takes a dictionary of file paths for the
        different GFC layers of a single tile ('lossyear', 'treecover2000',
        'datamask'). It first checks whether there is any data available
        (tile_files is None, returns None) or whether tree cover exists but no
        loss file is available (returns "all_zero"). Otherwise it opens each layer and
        merges them into a single xarray Dataset.

        Args:
            tile_files (dict[str, str | None] | None): A dictionary mapping GFC
                layer names to their file paths, or None if no files are available.

        Returns:
            xr.Dataset | None | Literal["all_zero"]: Either
                - an xarray Dataset containing the merged tile data
                - `None` in case of no data.
                - `"all_zero"` in case of no loss data.
        """
        # no data available
        if tile_files is None:
            return None
        if tile_files["lossyear"] is None:
            # no loss and no treecover is coded as NA - no action
            if rxr.open_rasterio(tile_files["treecover2000"]).sum() != 0:  # type: ignore
                # no loss data but treecover gets a zero (just one case in the data)
                return "all_zero"
            else:
                return None
        # read & combine tile data
        assert all(layer is not None for layer in tile_files), "Unexpected None in file_dict"
        das = [rxr.open_rasterio(tile_files[layer]).rename(layer) for layer in tile_files]  # type: ignore
        ds = xr.merge(das, compat="identical")
        return ds

    @staticmethod
    def _get_cell_tree_loss_share(pgid: int, years: list[int], ds: xr.Dataset) -> dict[int, float]:
        """Calculates the cumulative shares of tree loss compared to a single grid cell's total area.

        For a given grid cell, this static helper method first determines the
        baseline proportion of the cell's land area covered by trees in the year
        2000. It then iterates through a list of `years`, calculating
        the cumulative tree cover lost up to each year as a share of the initial
        2000 tree cover. This yearly loss share is then adjusted by the cell's
        overall tree cover proportion at the baseline.

        Args:
            pgid (int): The grid cell ID.
            years (list[int]): A list of years for which to calculate the cumulative
                tree loss share.
            ds (xr.Dataset): The xarray Dataset for the tile containing the grid
                cell, as created by `_combine_tile_data`.

        Returns:
            dict[int, float]: A dictionary where keys are years and values are
                the corresponding cumulative tree loss shares.
        """
        y, x = pgid_to_coords(pgid)
        ds_cell = ds.sel(x=slice(x - 0.25, x + 0.25), y=slice(y + 0.25, y - 0.25))
        # percent of grid cell land covered by trees
        cell_cover = ds_cell.treecover2000.where(ds_cell.datamask == 1).mean() / 100
        yearly_losses = {}
        for year in years:
            # dummy loss layer
            losses = xr.where(
                (ds_cell.lossyear > 0) & (ds_cell.lossyear <= int(str(year)[2:])), 1, 0
            )
            # account for differently covered pixels
            treecover_lost = (losses * ds.treecover2000).sum() / ds_cell.treecover2000.sum()
            # use percent cover as adjustment factor for cell
            treecover_adjusted = treecover_lost * cell_cover
            yearly_losses[year] = treecover_adjusted.item()
        return yearly_losses


if __name__ == "__main__":
    config = ConfigParser()
    grid = GlobalBaseGrid(config)
    indicator = VulEnvironmentalDeforestation(config=config, grid=grid)
    indicator.run()

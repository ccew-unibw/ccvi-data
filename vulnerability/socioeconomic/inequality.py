import ast
from functools import cache
import os

from concentrationMetrics import Index
import geopandas as gpd
from joblib import Parallel, delayed
import numpy as np
import pandas as pd
from panel_imputer import PanelImputer
import rioxarray as rxr
from rich.progress import Progress
import shapely
import xarray as xr

from base.datasets import WorldPopData, SWIIDData, NTLData, WPPData
from base.datasets.ntl import NTLDataError
from base.objects import Indicator, ConfigParser, GlobalBaseGrid
from utils.data_processing import slice_tuples
from utils.index import get_quarter
from utils.spatial_operations import assign_areas_to_grid


class VulSocioeconomicInequality(Indicator):
    requires_processing_storage: bool = True

    def __init__(
        self,
        config: ConfigParser,
        grid: GlobalBaseGrid,
        pillar: str = "VUL",
        dim: str = "socioeconomic",
        id: str = "inequality",
    ):
        """Params defining indicator's place in index set to designed hierarchy by default"""
        self.swiid = SWIIDData(config=config)
        self.ntl = NTLData(config=config)
        self.worldpop = WorldPopData(config=config)
        self.wpp = WPPData(config=config)
        self.data_config = config.get_data_config("admin1")
        super().__init__(pillar=pillar, dim=dim, id=id, config=config, grid=grid)

    def load_data(self) -> tuple[pd.DataFrame, gpd.GeoDataFrame, pd.DataFrame]:
        self.console.print("Loading required data...")
        df_swiid = self.swiid.load_data()
        adm1 = gpd.read_file(self.data_config["admin1"])
        self.worldpop.load_data()
        self.ntl.load_data()
        df_wpp = self.wpp.load_data()
        return df_swiid, adm1, df_wpp

    def preprocess_data(
        self, input_data: tuple[pd.DataFrame, gpd.GeoDataFrame, pd.DataFrame]
    ) -> pd.DataFrame:
        self.console.print("Preprocessing...")
        df_swiid, adm1, df_wpp = input_data
        df_swiid = self.swiid.preprocess_data(df_swiid)
        df_wpp = self.wpp.preprocess_wpp(df_wpp)
        adm1 = adm1.loc[adm1.shapeName != "Antarctica"]
        # build admin1 dataframe
        years = np.arange(self.global_config["start_year"], get_quarter("last").year + 1)
        max_year = min(years)  # initialize max year of available data
        adm1["year"] = [years for i in range(len(adm1))]
        adm1 = adm1.explode("year")
        adm1 = adm1.rename(columns={"shapeGroup": "iso3"})
        adm1 = adm1.merge(df_swiid.reset_index(), how="left", on=["iso3", "year"])
        adm1 = adm1.set_index(["shapeID", "year"]).sort_index()
        fp_ntl_gini = self.storage.build_filepath("processing", "adm1_ntl_gini_storage")
        if not self.regenerate["preprocessing"] and os.path.exists(fp_ntl_gini):
            adm1_prior = pd.read_parquet(fp_ntl_gini)
            adm1 = pd.merge(
                adm1, adm1_prior["ntl_gini"], how="left", left_index=True, right_index=True
            )
        else:
            adm1["ntl_gini"] = np.nan
        with Progress(console=self.console) as progress:
            task_gini = progress.add_task(
                "[blue]Generating NTL/WorldPop-based Adm1 Gini...", total=len(years)
            )
            for year in years:
                if adm1.loc[(slice(None), year), "ntl_gini"].isna().all():
                    self.console.print("Processing ntl and worldpop for year", year, "...")
                    try:
                        ds = self._combine_ntl_worldpop(year, df_wpp, progress)
                        self.console.print("Calculating adm1-level gini values...")
                        adm1.loc[(slice(None), year), "ntl_gini"] = (
                            adm1.loc[(slice(None), year), "geometry"]
                            .apply(lambda x: self._calculate_ntl_gini(ds, x))
                            .to_list()
                        )
                        ds.close()
                        max_year = year
                    except NTLDataError:
                        self.console.print(
                            f"NTL files for year {year} not (yet) available. Continuing with next year..."
                        )
                        adm1.loc[(slice(None), year), "ntl_gini"] = np.nan
                else:
                    self.console.print(
                        f"Preprocessed NTL/WorldPop-based adm1 gini values loaded from storage for year {year} ..."
                    )
                progress.update(task_gini, advance=1)
            self.storage.save(adm1[["ntl_gini"]], "processing", "adm1_ntl_gini_storage")
        # fill swiid to match ntl values for consistent time series
        imputer = PanelImputer(
            location_index="shapeID",
            time_index="year",
            imputation_method="interpolate",
            interp_method="linear",
            tail_behavior="fill",
            parallelize=True,
        )
        adm1["gini_disp"] = imputer.fit_transform(adm1["gini_disp"])  # type: ignore
        adm1.loc[
            (slice(None), slice(max_year + 1, adm1.index.get_level_values("year").max())),
            "gini_disp",
        ] = np.nan
        return adm1

    def create_indicator(self, df_preprocessed_adm1) -> pd.DataFrame:
        @cache
        def get_grid_gini(pgid: int, year: int) -> float:
            """
            reads value for column col for a given time and pgid
            """
            # all outside of shdi data
            if year > df_preprocessed_adm1.index.get_level_values("year").max():
                return np.nan
            elif pgid not in df_matching.index:
                return np.nan
            # here is the actual matching
            else:
                match = df_matching.loc[pgid, "admin_code"]
                if type(match) is dict:
                    temp = pd.DataFrame.from_dict(match, orient="index")
                    vals = df_preprocessed_adm1.loc[(temp.index, year), "gini_mean"]
                    val = np.average(vals.values, weights=temp.values.flatten())
                else:
                    val = df_preprocessed_adm1.loc[(match, year), "gini_mean"].item()
                assert isinstance(val, (np.floating, float))
                return val  # type: ignore

        df_indicator = self.create_base_df()
        df_preprocessed_adm1["gini_mean"] = df_preprocessed_adm1[["ntl_gini", "gini_disp"]].mean(
            axis=1
        )
        df_matching = self._build_grid_match_lookup()
        self.console.print("Matching admin1 values to the grid...")
        df_indicator[self.composite_id] = (
            df_indicator.reset_index()
            .apply(lambda x: get_grid_gini(x.pgid, x.year), axis=1)
            .to_list()
        )
        return df_indicator

    def normalize(self, df_indicator: pd.DataFrame) -> pd.DataFrame:
        """No normalization required for gini value."""
        return df_indicator[[self.composite_id]]

    def _combine_ntl_worldpop(
        self, year: int, df_wpp: pd.DataFrame, progress: Progress
    ) -> xr.Dataset:
        fp_out = self.storage.build_filepath("processing", f"ntl_pop_{year}.nc", filetype="")
        if os.path.exists(fp_out) and not self.regenerate["preprocessing"]:
            ntl = xr.open_dataset(fp_out)
            self.console.print("Existing harmonized NTL/WorlPop raster loaded from storage.")
        else:
            self.console.print(
                "Harmonizing NTL/WorlPop resolutions requires multiple hours per year."
            )
            ntl = self.ntl.get_masked_median_data(year)
            ntl["pop"] = xr.zeros_like(ntl.ntl)
            scale_factors = self.worldpop.prep_scale_factors(df_wpp)
            if year <= 2020:
                pop_files = self.worldpop.wp_files[year]
            else:
                pop_files = self.worldpop.wp_files[2020]
            task_combine = progress.add_task(
                "[yellow]Combine NTL and Worldpop...", total=len(pop_files) + 1
            )
            for f_pop in pop_files:
                iso3 = f_pop[f_pop.rfind("/") + 1 : f_pop.rfind("/") + 4].upper()
                fp = self.worldpop.storage.build_filepath("processing", f_pop, filetype="")
                pop_country = self._coarsen_match_ntl(fp, ntl)
                # apply scale factor
                pop_country = pop_country * scale_factors[year][iso3]
                pop_country = pop_country.round(0)
                # add to pop layer in ntl dataset - fill value require to keep data over iterations
                pop_country = pop_country.reindex_like(
                    ntl,
                    fill_value=0,  # type: ignore
                )  # expand to ntl dims for easy array arithmetic
                ntl["pop"] = ntl["pop"] + pop_country
                progress.update(task_combine, advance=1)
            enc = dict.fromkeys(list(ntl.keys()))
            for key in enc:
                enc[key] = {"zlib": True, "complevel": 3}
            ntl.to_netcdf(fp_out, encoding=enc)  # type: ignore
            progress.update(task_combine, advance=1)
            progress.remove_task(task_combine)
        return ntl

    def _calculate_ntl_gini(
        self, ds: xr.Dataset, geometry: shapely.Polygon | shapely.MultiPolygon
    ) -> float:
        indices = Index()
        minx, miny, maxx, maxy = geometry.bounds
        ds_clipped = ds.sel(
            x=slice(minx, maxx), y=slice(maxy, miny)
        )  # sel to bounds for faster clipping
        if any([len(ds_clipped.coords[coord]) == 0 for coord in ds.dims]):
            return np.nan
        else:
            try:
                ds_clipped = ds_clipped.rio.clip([geometry])
            except rxr.exceptions.NoDataInBounds:  # type: ignore
                return np.nan
            # filter bad values
            ds_clipped = ds_clipped.where(ds_clipped.ntl >= 0)
            ds_clipped["pop"] = ds_clipped["pop"].where(ds_clipped["pop"] > 0)
            # log to balance out that variance in lower light ranges is not as strong
            ds_clipped["ntl_pc"] = np.log1p(ds_clipped["ntl"]) / ds_clipped["pop"]
            ntl_pc = ds_clipped["ntl_pc"].values.flatten()
            ntl_pc = ntl_pc[~np.isnan(ntl_pc)]
            # increase threshold above mathematical minimum?
            if ntl_pc.size < 2 or all(ntl_pc == 0):
                ntl_gini = np.nan
            else:
                ntl_gini = indices.gini(ntl_pc)
        return ntl_gini

    def _build_grid_match_lookup(self) -> pd.DataFrame:
        """
        Creates the lookup file matching the adm1 shapes to the grid if it does not already exist
        and returns it.
        """
        self.console.print("Building grid - admin1 matching lookup file...")
        fp = self.storage.build_filepath("processing", "pgid_adm1_lookup", filetype=".csv")
        if os.path.exists(fp) and not self.regenerate["preprocessing"]:
            df = pd.read_csv(fp, index_col="pgid")
            df["admin_code"] = df.admin_code.apply(
                lambda x: ast.literal_eval(x) if x[0] == "{" else x
            )
            self.console.print("Existing lookup file found, skipping...")
        else:
            grid = self.grid.load(return_gdf=True)
            grid = assign_areas_to_grid(grid, self.data_config["admin1"], "admin_code", "shapeID")
            df = grid.drop(columns=["geometry", "iso3"])
            df.to_csv(fp)
        return df

    def _coarsen_match_ntl(self, fp: str, ntl: xr.Dataset) -> xr.DataArray:
        """
        Loads country population data from worldpop, reduced the resolution to the
        500m ntl resolution and matches coordinates with ntl data.

        Args:

        """
        # load pop data
        da = xr.open_dataset(fp)["pop_count"]
        pop_res = abs(da.x[1] - da.x[0]).item()
        # account for Northern/Southern NTL availability borders and drop areas outside from pop data
        if da.y.max() > ntl.y.max():
            da = da.sel(y=slice(ntl.y.max().item() + 2 * pop_res, None))
        if da.y.min() < ntl.y.min():
            da = da.sel(y=slice(None, ntl.y.min().item() - 2 * pop_res))

        # create country ntl view for calculation of target shape and to take coords
        ntl_country = ntl.sel(
            x=slice(
                self._find_nearest(ntl.x, da.x.min().item()),
                self._find_nearest(ntl.x, da.x.max().item()),
            ),
            y=slice(
                self._find_nearest(ntl.y, da.y.max().item()),
                self._find_nearest(ntl.y, da.y.min().item()),
            ),
        )
        # calculate necessary paddings for coarsen
        x_pad = ntl_country.x.shape[0] * 5 - da.x.shape[0]
        y_pad = ntl_country.y.shape[0] * 5 - da.y.shape[0]
        assert x_pad >= 0, y_pad >= 0

        # pixels to pad is sum of default padding of 2 (when pixel centers at the boundaries roughly match)
        # and +/- difference in number of pop pixels between array boundaries
        diff_xmin = (da.x.min() - ntl_country.x.min()).item()
        diff_xmax = (ntl_country.x.max() - da.x.max()).item()
        x_pad0 = 2 + round(diff_xmin / pop_res)
        x_pad1 = 2 + round(diff_xmax / pop_res)
        assert x_pad0 + x_pad1 == x_pad

        diff_ymin = (da.y.min() - ntl_country.y.min()).item()
        diff_ymax = (ntl_country.y.max() - da.y.max()).item()
        y_pad0 = 2 + round(diff_ymax / pop_res)  # 0 is max for y since lat goes from large to small
        y_pad1 = 2 + round(diff_ymin / pop_res)
        assert y_pad0 + y_pad1 == y_pad

        # reduce resolution to ntl
        da_coarse = self._coarsen_in_chunks(da, y_pad0, y_pad1, x_pad0, x_pad1)
        da.close()
        assert da_coarse.shape == ntl_country.ntl.shape
        da_coarse = da_coarse.assign_coords(ntl_country.coords)
        return da_coarse

    def _find_nearest(self, coords: np.ndarray, value: float) -> float:
        array = coords.values  # type: ignore
        idx = (np.abs(array - value)).argmin()
        return array[idx]

    def _coarsen_in_chunks(
        self,
        da: xr.DataArray,
        y_pad0: float,
        y_pad1: float,
        x_pad0: float,
        x_pad1: float,
        chunksize: int = 10000,
    ) -> xr.DataArray:
        def par_func_chunk_processing(i: int, j: int) -> xr.DataArray:
            """i - x index, j - y index"""
            # set slice start and ends
            x_start = da_x_chunks[i]
            if i == len(da_x_chunks) - 1:
                x_end = None
            else:
                x_end = da_x_chunks[i + 1]
            y_start = da_y_chunks[j]
            if j == len(da_y_chunks) - 1:
                y_end = None
            else:
                y_end = da_y_chunks[j + 1]
            chunk = da[slice(y_start, y_end), slice(x_start, x_end)].load()
            # apply padding: left/right boundary
            if i == 0:
                chunk = chunk.pad(x=(x_pad0, 0))
            if i == len(da_x_chunks) - 1:
                chunk = chunk.pad(x=(0, x_pad1))
            # apply padding: upper/lower boundary
            if j == 0:
                chunk = chunk.pad(y=(y_pad0, 0))
            if j == len(da_y_chunks) - 1:
                chunk = chunk.pad(y=(0, y_pad1))

            # coarsen chunks and add to list
            chunk = chunk.coarsen(x=5, y=5, boundary="exact").sum()  # type: ignore
            return chunk

        da_x_chunks = np.arange(0, da.x.shape[0], chunksize) - x_pad0
        da_x_chunks[0] = 0
        da_y_chunks = np.arange(0, da.y.shape[0], chunksize) - y_pad0
        da_y_chunks[0] = 0

        # process chunks with parallelization
        # memory usage could be reduced by reducing n_jobs if there are any issues
        das = Parallel(n_jobs=6, verbose=0)(
            delayed(par_func_chunk_processing)(i, j)
            for i, j in slice_tuples(da_x_chunks, da_y_chunks)
        )
        pop_coarsened: xr.DataArray = xr.combine_by_coords(das)[da.name]
        return pop_coarsened


# this is possible by adding the root folder as the PYTHONPATH var in .env
if __name__ == "__main__":
    config = ConfigParser()
    grid = GlobalBaseGrid(config)
    indicator = VulSocioeconomicInequality(config=config, grid=grid)
    indicator.run()
